Instructions:

Redshift setup and usage:
Reference Link:
https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html

--Create a bucket in s3 and upload data from above mentioned link (tickit)

Options:
--Create cluster (for more granular control)
name: rs-cluster-1
size of cluster: dc2.large
number of nodes: 2
admin user name: awsuser
pswd: abcd@1234

--create default IAM role > Any s3 bucket
  Add policy to have AmazonRedshiftAllCommandsFullAccess
Additional configurations: use default

---cluster created with public access option ( and then connect to it using external client such as sql workbench)
---cluster created without public access (connect to it using internal client or sql editor: 
   will also need right roles to read/write data from s3]

--Create roles that allows redshift to access data from s3 (AmazonS3FullAccess) & have (AmazonRedshiftAllCommandsFullAccess)
--Create redshift cluster [without allowing public access and this can be edited later]


Get the jdbc url
jdbc:redshift://redshift-cluster-xxxxxxxxxxxxxxxx.eu-central-1.redshift.amazonaws.com:5439/dev

Get the role arn from role you just created (will be needed when we load data from s3 to redshift )
arn:aws:iam::381492xxxxx:role/service-role/AmazonRedshift-CommandsAccessRole-2024031xxxxxxxxx

Test your connection
Amazon Redshift retains a great deal of metadata about the various databases within a cluster and finding a 
list of tables is no exception to this rule.

--check if following permissions are added (if needed)
AmazonRedshiftQueryEditorV2FullAccess

The most useful object for this task is the PG_TABLE_DEF table, which as the name implies, 
contains table definition information. Note: The PG_ prefix is just a holdover from PostgreSQL, 
the database technology from which Amazon Redshift was developed.
To begin finding information about the tables in the system, you can simply return columns from PG_TABLE_DEF:

SELECT
  *
FROM
  PG_TABLE_DEF;

For better or worse, PG_TABLE_DEF contains information about everything in the system, 
so the results of such an open query will be massive, but should give you an idea of what 
PG_TABLE_DEF is capable of

To limit the results to user-defined tables, it’s important to specify the schemaname column to return 
only results which are public:

SELECT distinct(schemaname) FROM   PG_TABLE_DEF;

Lastly, if we are solely interested only the names of tables which are user-defined, 
we’ll need to filter the above results by retrieving DISTINCT items from within the tablename column:

SELECT
  DISTINCT tablename FROM PG_TABLE_DEF WHERE schemaname = 'pg_catalog';
--This returns only the unique tables within the system:

--creating tables
create table users(
	userid integer not null distkey sortkey,
	username char(8),
	firstname varchar(30),
	lastname varchar(30),
	city varchar(30),
	state char(2),
	email varchar(100),
	phone char(14),
	likesports boolean,
	liketheatre boolean,
	likeconcerts boolean,
	likejazz boolean,
	likeclassical boolean,
	likeopera boolean,
	likerock boolean,
	likevegas boolean,
	likebroadway boolean,
	likemusicals boolean);

create table venue(
	venueid smallint not null distkey sortkey,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer);

create table category(
	catid smallint not null distkey sortkey,
	catgroup varchar(10),
	catname varchar(10),
	catdesc varchar(50));

create table date(
	dateid smallint not null distkey sortkey,
	caldate date not null,
	day character(3) not null,
	week smallint not null,
	month character(5) not null,
	qtr character(5) not null,
	year smallint not null,
	holiday boolean default('N'));

create table event(
	eventid integer not null distkey,
	venueid smallint not null,
	catid smallint not null,
	dateid smallint not null sortkey,
	eventname varchar(200),
	starttime timestamp);

create table listing(
	listid integer not null distkey,
	sellerid integer not null,
	eventid integer not null,
	dateid smallint not null  sortkey,
	numtickets smallint not null,
	priceperticket decimal(8,2),
	totalprice decimal(8,2),
	listtime timestamp);

create table sales(
	salesid integer not null,
	listid integer not null distkey,
	sellerid integer not null,
	buyerid integer not null,
	eventid integer not null,
	dateid smallint not null sortkey,
	qtysold smallint not null,
	pricepaid decimal(8,2),
	commission decimal(8,2),
	saletime timestamp);


--check for tables:
SELECT   *  FROM   PG_TABLE_DEF where tablename = 'venue';
SELECT   *  FROM   PG_TABLE_DEF where schemaname = 'public';

--Lets upload data to redshift tables from s3:
--Download files from tickit folder that contains individual sample data files. 
--Load files in your Amazon S3 bucket in your AWS Region.
--Edit the COPY commands to point to the files in your Amazon S3 bucket.

--copy data from s3 
COPY users 
FROM 's3://buck1mar/allusers_pipe.txt' 
DELIMITER '|' 
TIMEFORMAT 'YYYY-MM-DD HH:MI:SS'
REGION 'eu-central-1'
IAM_ROLE '<your role arn or default>'                    
                    
COPY event
FROM 's3://buck1mar/allevents_pipe.txt' 
DELIMITER '|' 
TIMEFORMAT 'YYYY-MM-DD HH:MI:SS'
IGNOREHEADER 0 
REGION 'eu-central-1'
IAM_ROLE default;

--error: Load into table 'event' failed. Check 'sys_load_error_detail' system table for details. 
select * from sys_load_error_detail
--shows: Invalid digit, Value 'e', Pos 0, Type: Integer  
--issue with file

--retry
COPY dev.public.event FROM 's3://buck1mar/allevents_pipe.txt' 
IAM_ROLE '<your role arn or default>' 
FORMAT AS CSV DELIMITER '|' QUOTE '"' REGION AS 'eu-central-1'

COPY sales
FROM 's3://buck1mar/sales_tab.txt' 
DELIMITER '\t' 
TIMEFORMAT 'MM/DD/YYYY HH:MI:SS'
REGION 'eu-central-1'
IAM_ROLE '<your role arn or default>';  

COPY category
FROM 's3://buck1mar/category_pipe.txt' 
DELIMITER '|' 
TIMEFORMAT 'MM/DD/YYYY HH:MI:SS'
REGION 'eu-central-1'
IAM_ROLE '<your role arn or default>';   

--if when loading for listing any error, look into
select * from sys_load_error_detail
--Invalid timestamp format or value [MM/DD/YYYY HH:MI:SS]

--try again
COPY dev.public.listing FROM 's3://buck1mar/listings_pipe.txt' 
IAM_ROLE '<your role arn or default>' 
FORMAT AS CSV DELIMITER '|' QUOTE '"' IGNOREHEADER 0 REGION AS 'eu-central-1'

COPY dev.public.date FROM 's3://buck1mar/date2008_pipe.txt' 
IAM_ROLE '<your role arn or default>' 
FORMAT AS CSV DELIMITER '|' QUOTE '"' REGION AS 'eu-central-1'

COPY dev.public.venue FROM 's3://buck1mar/venue_pipe.txt' 
IAM_ROLE '<your role arn or default>' 
FORMAT AS CSV DELIMITER '|' QUOTE '"' REGION AS 'eu-central-1'

--or using query editor v2 > right click on cluster > create connection > 
--using database username > and load manually

--since I added _ajy as suffix to the above tables while creating

#--check if user is superuser
--if your user is privileged with the result of the usesuper
SELECT usename, usesysid, usesuper FROM pg_user WHERE usename=current_user;

select count(*) from users_ajy;
select count(*) from venue_ajy;
select count(*) from category_ajy;
select count(*) from date_ajy;
select count(*) from event_ajy;
select count(*) from listing_ajy;
select count(*) from sales_ajy;

---sample queries--------------

Querying your data/analysis
-- Find top 10 buyers by quantity.
SELECT firstname, lastname, total_quantity 
FROM   (SELECT buyerid, sum(qtysold) total_quantity
        FROM  sales
        GROUP BY buyerid
        ORDER BY total_quantity desc limit 10) Q, users
WHERE Q.buyerid = userid
ORDER BY Q.total_quantity desc;

Note** Prepend 'explain' before query to look at explain results and 
       what happened behind the scenes.
       --we can also find this information from 'query monitoring' in console.

-- Find events in the 99.9 percentile in terms of all time gross sales.
SELECT eventname, total_price 
FROM  (SELECT eventid, total_price, ntile(1000) over(order by total_price desc) as percentile 
       FROM (SELECT eventid, sum(pricepaid) total_price
             FROM   sales
             GROUP BY eventid)) Q, event E
       WHERE Q.eventid = E.eventid
       AND percentile = 1
ORDER BY total_price desc;

-- Get definition for the sales table.
SELECT *    
FROM pg_table_def    
WHERE tablename = 'sales';    

-- Find total sales on a given calendar date.
SELECT sum(qtysold) 
FROM   sales, date 
WHERE  sales.dateid = date.dateid 
AND    caldate = '2008-01-05';

-- Find top 10 buyers by quantity.
SELECT firstname, lastname, total_quantity 
FROM   (SELECT buyerid, sum(qtysold) total_quantity
        FROM  sales
        GROUP BY buyerid
        ORDER BY total_quantity desc limit 10) Q, users
WHERE Q.buyerid = userid
ORDER BY Q.total_quantity desc;

-- Find events in the 99.9 percentile in terms of all time gross sales.
SELECT eventname, total_price 
FROM  (SELECT eventid, total_price, ntile(1000) over(order by total_price desc) as percentile 
       FROM (SELECT eventid, sum(pricepaid) total_price
             FROM   sales
             GROUP BY eventid)) Q, event E
       WHERE Q.eventid = E.eventid
       AND percentile = 1
ORDER BY total_price desc;

& so on...

----Sample queries end-----------------
=================================================
#System catalog tables
PG_*

The system catalogs store schema metadata, such as information about tables and columns. 
System catalog tables have a PG prefix.

PG_ATTRIBUTE_INFO
--Amazon Redshift system view built on the PostgreSQL catalog table PG_ATTRIBUTE and the internal catalog table PG_ATTRIBUTE_ACL.
PG_ATTRIBUTE_INFO includes details about columns of a table or view, including column access control lists, if any.

PG_CLASS_INFO
-- view built on the PostgreSQL catalog tables PG_CLASS and PG_CLASS_EXTENDED.
  PG_CLASS_INFO includes details about table creation time and the current distribution style. 

PG_DATABASE_INFO
--view that extends the PostgreSQL catalog table PG_DATABASE.

PG_DEFAULT_ACL
--Stores information about default access privileges.

PG_EXTERNAL_SCHEMA
--Stores information about external schemas.

PG_LIBRARY
--Stores information about user-defined libraries.

PG_PROC_INFO
--view built on the PostgreSQL catalog table PG_PROC and the internal catalog table PG_PROC_EXTENDED. 
PG_PROC_INFO includes details about stored procedures and functions, including information related to output arguments, if any.

PG_STATISTIC_INDICATOR
--Stores information about the number of rows inserted or deleted since the last ANALYZE.
The PG_STATISTIC_INDICATOR table is updated frequently following DML operations, so statistics are approximate.

PG_TABLE_DEF
--Stores information about table columns.

PG_TABLE_DEF only returns information about tables that are visible to the user.
If PG_TABLE_DEF does not return the expected results, verify that the search_path parameter is set correctly to include the relevant schemas.
You can use SVV_TABLE_INFO to view more comprehensive information about a table, including data distribution skew, 
key distribution skew, table size, and statistics.

PG_USER_INFO
--view that shows user information, such as user ID and password expiration time.

==================
--Look into other important catalog/system tables
#System monitoring tables  (but for provisioned only clusters)
#STL views are generated from logs that have been persisted to disk to provide a history of the system.
STL* 

#SVV - metadata views
Contain information about database objects with references to transient STV tables.
select * from SVV_TABLES ;
select * from SVV_TABLES where table_name like 'venue%';

#SYS - monitoring views
used to monitor query and workload usage for provisioned clusters and serverless workgroups.

#STV tables are virtual system tables that contain snapshots of the current system data. 
 They are based on transient in-memory data and are not persisted to disk-based logs or regular tables.
--using STV tables
--cancelling a long running query
select pid,trim(user_name),starttime,substring(query,1,20) from stv_recents where status = 'Running';
cancel pid;
abort or rollback;

#SVCS views provide details about queries on both the main and concurrency scaling clusters.

#SVL views provide details about queries on main clusters.

--Examples
#SVV_TABLE_INFO
Shows summary information for tables in the database. 
The view filters system tables and shows only user-defined tables.

select * from SVV_TABLE_INFO ;
select * from SVV_TABLE_INFO WHERE "TABLE" like '%ajy%';

The SVV_TABLE_INFO view summarizes information 
from the STV_BLOCKLIST, STV_NODE_STORAGE_CAPACITY, STV_TBL_PERM, and STV_SLICES system tables.

#STV_BLOCKLIST
--contains the number of 1 MB disk blocks that are used by each slice, table, or column in a database.
--Every table you create has three hidden columns appended to it: INSERT_XID, DELETE_XID, and ROW_ID (OID)
--get the table_id (=tbl) from SVV_TABLE_INFO
Note** contains
minvalue: Minimum data value of the block. Stores first eight characters as 64-bit integer 
          for non-numeric data. Used for disk scanning.
maxvalue: Maximum data value of the block. Stores first eight characters as 64-bit integer 
          for non-numeric data. Used for disk scanning.
sb_pos: Internal Amazon Redshift identifier for super block position on the disk.
pinned: If the block is pinned into memory as part of pre-load. 0 = false; 1 = true. Default is false.
on_disk: If the block is automatically stored on disk. 0 = false; 1 = true. Default is false.
unsorted: If a block is unsorted. 0 = false; 1 = true. Default is true.
num_values: Number of values contained on the block.

select * from STV_BLOCKLIST where "tbl" = 106406;

select * from pg_table_def where tablename like '%_aj'; --encoding, distkey, sortkey
select * from svv_table_info  where "table" like '%_aj'; --encoded, diststyle , sortkey, unsorted, tbl_rows, skew_rows

#STV_NODE_STORAGE_CAPACITY
select * from STV_NODE_STORAGE_CAPACITY;

#STV_TBL_PERM
--information about the permanent tables in Amazon Redshift, 
  including temporary tables created by a user for the current session. 
--get the table_id (=id) from SVV_TABLE_INFO

select * from STV_TBL_PERM;
select * from STV_TBL_PERM  where id = 106406;

--get tableIDs for each table
select distinct id, name
from stv_tbl_perm order by name;

--To determine the number of blocks used by each column in the any table
select col, count(*)
from stv_blocklist, stv_tbl_perm
where stv_blocklist.tbl = stv_tbl_perm.id
and stv_blocklist.slice = stv_tbl_perm.slice
and stv_tbl_perm.name = 'listing_ajy'
group by col
order by col;

--or we can
select a.slice,a.col,b.block_count, b.rows
from stv_blocklist a, stv_tbl_perm b
where a.tbl = b.id
and a.slice = b.slice
and b.name = 'listing_aj' and a.slice = 1;

--checking for consistency which creates a uniformly distributed dataset
select  sellerid,count(*) 
from listing GROUP BY 1 ORDER BY 2 DESC; 

--how many values are assigned to each slice

select count(*) from listing;
select * from stv_tbl_perm where name = 'listing_ajy';

--how many values are assigned to each slice

SELECT rows/15 assigned_values, COUNT(*) number_of_slices, slice FROM stv_tbl_perm WHERE name='listing_ajy'
GROUP BY 1,3 ORDER BY 1;

or

SELECT name, slice, rows, COUNT(*) number_of_slices FROM stv_tbl_perm WHERE name='venue_ajy' or name = 'listing_ajy'
GROUP BY 1,2,3 ORDER BY 3 desc;

Note**
The ROWS column includes counts of deleted rows that have not been vacuumed (or have been vacuumed but with the SORT ONLY option). 
Therefore, the SUM of the ROWS column in the STV_TBL_PERM table might not match the COUNT(*) result when you query 

for example:
delete from venue
where venueid in (1,2);
select count(*) from venue;

&
select trim(name) tablename, sum(rows)
from stv_tbl_perm where name='venue' group by name;

--To synchronize the data in STV_TBL_PERM, run a full vacuum
vacuum venue;

--now try again
select trim(name) tablename, sum(rows)
from stv_tbl_perm
where name='venue'
group by name;

#STV_SLICES
--the current mapping of a slice to a node. Type field is for internal use only.
select * FROM STV_SLICES;

#STV_PARTITIONS
-- to find out the disk speed performance and disk utilization for Amazon Redshift.
-- contains one row per node per logical disk volume
select * FROM STV_PARTITIONS;

#STL_SCAN
--Analyzes table scan steps for queries. 
The step number for rows in this table is always 0 because a scan is the first step in a segment.
It contains queries only on main clusters and not on concurrency scaling clusters.
select * from  STL_SCAN;

#STL_CONNECTION_LOG
--Logs authentication attempts and connections and disconnections.
select * from stl_connection_log
where event = 'initiating session'
and pid not in
(select pid from stl_connection_log
where event = 'disconnecting session')
order by 1 desc;

#STL_QUERY
--The STL_QUERY and STL_QUERYTEXT views only contain information about queries
select * from STL_QUERY ;
select * from STL_QUERY_HISTORY;

Note**
--For a listing and information on all statements run , 
      you can also query the STL_DDLTEXT and STL_UTILITYTEXT views
--For a listing of all statements run , you can query the SVL_STATEMENTTEXT view.

--long running queries
select * from stv_inflight; --get the PID
select PG_TERMINATE_BACKEND(1073930346)

======================

--Test setting up 'Amazon Redshift Serverless' > WG > NS
