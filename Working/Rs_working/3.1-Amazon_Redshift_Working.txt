#Additional user:
create group tstgroup;
create user tstuser1 in group tstgroup password 'Abcd$1234';
grant all privileges on all tables in schema public to tstuser1;

--we can also make newuser as owner of this schema or create a new schema

#Setting up serverless redshift
-----------------------
Try redshift serverless > 
--use default settings or customize
If customize settings
--Namespace - a collection of database objects and users
namespace: mynsredshift
--dbname: dev
--customize admin user credentials : 
admin
Abcd$1234
--Associate IAM role
Create IAM role as default , able to access any s3 bucket.
--we need to add 'AmazonRedshiftAllCommandsFullAccess' & 's3 full access' policy to this role.

--Security and encrytion (default)
--workgroup (a collection of compute resources from which an endpoint is created,
		Compute properties include network and security settings.)
workgroup name: mywgforredshift

Capacity: Capacity is measured in Redshift processing units (RPUs). (used for processing workloads)
          Setting the RPU value higher increases capability and improves performance.
from 8-512 [in increments of 8], default being 128.

--Network and security
  use default vpc
  use default security group and edit ingree/egress rules to control in/out traffic.

<save configuration>
--setting up your 'Amazon Redshift Serverless'

Other Info:
Total snapshots:
Datashares in my account:
Datashares requiring authorization
Datashares from other accounts
Datashares requiring assoiciation.

--to visualize cost of your total compute usage > AWS cost explorer.

Query editor >
--create table 
create table event(
	eventid integer not null distkey,
	venueid smallint not null,
	catid smallint not null,
	dateid smallint not null sortkey,
	eventname varchar(200),
	starttime timestamp);

--This gets created in dev db, public schema
SELECT * FROM PG_TABLE_DEF;
SELECT * FROM PG_TABLE_DEF where schemaname = 'public';
SELECT * FROM PG_TABLE_DEF  where tablename = 'event'

--check your role 'AmazonRedshift-CommandsAccessRole-2024xxxxxx' & permissions/policies attached to it.

Now upload a file from github into your s3 bucket (buck1mar--create it if not created)
filename: allevents_pipe-1.txt

note**file contains 8799 rows plus header row.
If file did not contain header, we can avoid the 'IGNOREHEADER' option.
file contains '|' as delimiter or use delimiter as in file.

COPY event
FROM 's3://buck1mar/allevents_pipe-1.txt' 
DELIMITER '|' 
TIMEFORMAT 'YYYY-MM-DD HH:MI:SS'
IGNOREHEADER 1
REGION 'eu-central-1'
IAM_ROLE 'arnxxxx';

select count(*) from dev.public.event;
8798 rows

delete from dev.public.event;

select count(*) from dev.public.event;
0

drop table..
-------------
<optional> Note** its same as in provisioned cluster.
create tables -users,event,venue,category,date,listing & sales.
Load data from files in buck1mar.
---------------
Understanding RPUs
Earlier the minimum base capacity required to run serverless : 32 RPU. 
New lowered base capacity  : 8 RPU
incrementing with 8 RPUs

From left menu > workgroup configuration 
--here we can modify 'data access' settings (later..)
--here we can modify 'limits' to control base capacity (i.e. RPUs), Max capacity & usage limits.
  --we can also add multiple 'query limits' from 'Manage query limits'.

----------------------------------
create database mydb;
select * from pg_database;
create user myuser password 'Abcd$1234';
select * from pg_user;
select CURRENT_USER;
select CURRENT_SCHEMA();

--we can create multiple schemas within the same database to organize data
create SCHEMA myschema AUTHORIZATION myuser;
select * from pg_namespace;
grant select on all tables in schema myschema to myuser;
grant all on schema myschema to myuser;

--while being in dev if we try
create table mydb.public.emp1(empid int, name varchar(max));
Err: cross-database reference to database "mydb" is not supported
--switch db to mydb and rerun
create table mydb.public.emp1(empid int, name varchar(max));

--similary switch to dev
create table dev.myschema.emp1(empid int, name varchar(max));

show search_path;
SELECT * FROM PG_TABLE_DEF where schemaname = 'myschema';
set search_path to '$user','public','myschema';
SELECT * FROM PG_TABLE_DEF where schemaname = 'myschema';

--Testing exlain
create table event (--from query given above)
load data into table (--as shown above using file 'allevents_pipe.txt'

select eventname,count(*) from event group by eventname;
explain select eventname,count(*) from event group by eventname;
enable explain from menu on top to look into graph.

#Using compression
https://aws.amazon.com/blogs/aws/data-compression-improvements-in-amazon-redshift/

create table venue3(
	venueid smallint not null encode az64 ,
	venuename varchar(100) encode bytedict,
	venuecity varchar(30) encode bytedict,
	venuestate char(2) encode lzo,
	venueseats integer encode az64)
 sortkey(venueid,venuecity);

select * from pg_table_def where tablename = 'venue3';

alter table venue3 alter column venuename encode lzo;
alter table venue3 alter column venuecity encode zstd;

select * from pg_table_def where tablename = 'venue3';

--Testing ANALYZE COMPRESSION
Performs compression analysis and produces a report with the suggested compression encoding for the tables analyzed. 
For each column, the report includes an estimate of the potential reduction in disk space compared to the current encoding.(if encoding was explicitly
chosen)

analyze compression venue3;

select count(*) from venue;
select count(*) from venue3;

select * from venue;

--querying tables and using columns based on distkey,sortkey and also columns which have compressions.
--check which compression gives a better performance.
explain select * from venue where venueid > 25 and venuecity = 'Orlando';
explain select * from venue3 where venueid > 25 and venuecity = 'Orlando';

--look at elapsed time and explain result.

#understanding distkey/sortkey or both
--create venue table if not created already.
create table venue(
	venueid smallint not null distkey sortkey,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer);
note** usage of distkey and sortkey

--no distkey
create table venue1(
	venueid smallint not null ,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer)
    sortkey(venueid);

--multiple column sortkey

create table venue2(
	venueid smallint not null ,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer)
 sortkey(venueid,venuecity);

select * from pg_table_def where tablename = 'venue2';
--look at columns distkey and sortkey(for value other than 0)

COPY dev.public.venue 
FROM 's3://buck1mar/venue_pipe.txt' 
IAM_ROLE 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502' 
FORMAT AS CSV DELIMITER '|'
REGION AS 'eu-central-1'

elapsed time: 1.6s

load same data into venue2 and check elapsed time (~25s)
--second run : 13s
select count(*) from venue;
select count(*) from venue2;

select * from venue where venueid = 7 and venuecity = 'Toronto';
xxxx ms
select * from venue2 where venueid = 7 and venuecity = 'Toronto';
yyyy ms (lesser than xxxx?)

--multiple runs reduces the time taken..

<optional>
create table venue4(
	venueid smallint not null ,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer ) 
    sortkey auto ;

select * from pg_table_def where tablename = 'venue4';
----------
#--create listing table
create table listing(
	listid integer not null distkey,
	sellerid integer not null,
	eventid integer not null,
	dateid smallint not null  sortkey,
	numtickets smallint not null,
	priceperticket decimal(8,2),
	totalprice decimal(8,2),
	listtime timestamp);

create table listing2(
	listid integer not null,
	sellerid integer not null,
	eventid integer not null,
	dateid smallint not null,
	numtickets smallint not null,
	priceperticket decimal(8,2),
	totalprice decimal(8,2),
	listtime timestamp) ;

select * from svv_table_info where "table" like 'listing%'

SELECT name, slice, rows, COUNT(*) number_of_slices FROM stv_tbl_perm 
WHERE name like '%ajy%'

select "column", type, encoding, distkey, sortkey 
from pg_table_def where tablename = 'listing_ajy';

select slice, col, num_values as rows, minvalue, maxvalue
from svv_diskusage
where name='listing_ajy' and col=0 and rows>0
order by slice, col;

select * from listing_ajy where listid > 1315 limit 1000;
xxxx ms
select * from listing_ajy where listid > 1315 limit 1000;
xxxx ms

SELECT listid, count(*) FROM listing_ajy GROUP BY listid order by listid desc;
SELECT eventid, count(*) FROM listing_ajy GROUP BY eventid order by count desc;
SELECT sellerid, count(*) FROM listing_ajy GROUP BY sellerid order by count desc;

--look at count of records based on these columns to decide on best distkey or let system decide as it did in 'listing2_ajy'
Note**
Each node must aggregate its own rows first; then, the leader node has to aggregate the results again. 
That's why we see two "Aggregate" steps for a query.
To avoid large amount of data being sent to leader node over network, we can put all rows belonging to an ID or group
by using right distkey.
To collocate all relevant rows in a single node, use the distkey on a column which will be part of your queries or grupby queries.

Note**
If too many rows land onto a single node, it is the so-called skew. When such a skew occurs, the total query processing 
time takes much longer because the performance is capped by the slowest processing node; 
i.e., the query cannot be spread across multiple nodes. 
In this (extreme) case, almost all the rows were processed by a single node. 
That is why the query may take longer than the query made against the table without a DISTKEY.

--test
create table listing3(
	listid integer not null,
	sellerid integer not null,
	eventid integer not null,
	dateid smallint not null  sortkey,
	numtickets smallint not null,
	priceperticket decimal(8,2),
	totalprice decimal(8,2),
	listtime timestamp) diststyle key distkey (sellerid);

--Load data into both tables
COPY dev.public.listing2 FROM 's3://buck1mar/listings_pipe.txt' 
IAM_ROLE 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502' 
FORMAT AS CSV DELIMITER '|' QUOTE '"' REGION AS 'eu-central-1'

Select diststyle, skew_rows from svv_table_info ;
Select diststyle, skew_rows from svv_table_info where "table" = 'listing2';

--test querying on bth tables..
---------
--Unload data to S3
unload ('select * from listing') to 's3://buck1mar/listing_bckup'
iam_role 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502'
json

Note** creates multiple files here with prefix as listing_backup..
Amazon Redshift splits the results of a select statement across a set of files, one or more files per node slice, 
to simplify parallel reloading of the data. Alternatively, you can specify that UNLOAD should write the results serially to 
one or more files by adding the PARALLEL OFF option. You can limit the size of the files in Amazon S3 by specifying the MAXFILESIZE parameter. 
UNLOAD automatically encrypts data files using Amazon S3 server-side encryption (SSE-S3).

By default, UNLOAD writes data in parallel to multiple files, according to the number of slices in the cluster

unload ('select * from listing') to 's3://buck1mar/listing_bckup2'
iam_role 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502'
parallel off

other options:
--to divide data into equal size files
maxfilesize xx mb;

--to create a manifest file that lists the unload files
The manifest is a text file in JSON format that explicitly lists the URL of each file that was written to Amazon S3.
manifest

--UNLOAD command includes a quoted string in the select statement, so the quotation marks are escaped (=\'OH\' ').
unload ('select venuename, venuecity from venue where venuestate=\'OH\' ')
to 's3://buck1mar/listing_bckup'
iam_role 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502'

--to overwrite existing files
allowoverwrite
==========================























