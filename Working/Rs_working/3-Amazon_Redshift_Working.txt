#Setting up serverless redshift
-----------------------
Try redshift serverless > 
--use default settings or customize
If customize settings
--Namespace - a collection of database objects and users
namespace: mynsredshift
--dbname: dev
--customize admin user credentials : 
admin
Abcd$1234
--Associate IAM role
Create IAM role as default , able to access any s3 bucket.
--we need to add 'AmazonRedshiftAllCommandsFullAccess' & 's3 full access' policy to this role.

--Security and encrytion (default)
--workgroup (a collection of compute resources from which an endpoint is created,
		Compute properties include network and security settings.)
workgroup name: mywgforredshift

Capacity: Capacity is measured in Redshift processing units (RPUs). (used for processing workloads)
          Setting the RPU value higher increases capability and improves performance.
from 8-512 [in increments of 8], default being 128.

--Network and security
  use default vpc
  use default security group and edit ingree/egress rules to control in/out traffic.

<save configuration>
--setting up your 'Amazon Redshift Serverless'

Other Info:
Total snapshots:
Datashares in my account:
Datashares requiring authorization
Datashares from other accounts
Datashares requiring assoiciation.

--to visualize cost of your total compute usage > AWS cost explorer.

Query editor >
--create table 

create table event(
	eventid integer not null distkey,
	venueid smallint not null,
	catid smallint not null,
	dateid smallint not null sortkey,
	eventname varchar(200),
	starttime timestamp);

--This gets created in dev db, public schema
SELECT * FROM PG_TABLE_DEF;
SELECT * FROM PG_TABLE_DEF where schemaname = 'public';
SELECT * FROM PG_TABLE_DEF  where tablename = 'event'

--check your role 'AmazonRedshift-CommandsAccessRole-2024xxxxxx' & permissions/policies attached to it.

Now upload a file from github into your s3 bucket (buck1mar--create it if not created)
filename: allevents_pipe-1.txt

note**file contains 8799 rows plus header row.
If file did not contain header, we can avoid the 'IGNOREHEADER' option.
file contains '|' as delimiter or use delimiter as in file.

COPY event
FROM 's3://buck1mar/allevents_pipe-1.txt' 
DELIMITER '|' 
TIMEFORMAT 'YYYY-MM-DD HH:MI:SS'
IGNOREHEADER 1
REGION 'eu-central-1'
IAM_ROLE 'arnxxxx';

select count(*) from dev.public.event;
8798 rows

delete from dev.public.event;

select count(*) from dev.public.event;
0

drop table..
-------------
<optional> Note** its same as in provisioned cluster.
create tables -users,event,venue,category,date,listing & sales.
Load data from files in buck1mar.
---------------
Understanding RPUs
Earlier the minimum base capacity required to run serverless : 32 RPU. 
New lowered base capacity  : 8 RPU
incrementing with 8 RPUs

From left menu > workgroup configuration 
--here we can modify 'data access' settings (later..)
--here we can modify 'limits' to control base capacity (i.e. RPUs), Max capacity & usage limits.
  --we can also add multiple 'query limits' from 'Manage query limits'.

----------------------------------

create database mydb;
select * from pg_database;
create user myuser password 'Abcd$1234';
select * from pg_user;
select CURRENT_USER;
select CURRENT_SCHEMA();

--we can create multiple schemas within the same database to organize data
create SCHEMA myschema AUTHORIZATION myuser;
select * from pg_namespace;
grant select on all tables in schema myschema to myuser;
grant all on schema myschema to myuser;

--while being in dev if we try
create table mydb.public.emp1(empid int, name varchar(max));
Err: cross-database reference to database "mydb" is not supported
--switch db to mydb and rerun
create table mydb.public.emp1(empid int, name varchar(max));

--similary switch to dev
create table dev.myschema.emp1(empid int, name varchar(max));

show search_path;
SELECT * FROM PG_TABLE_DEF where schemaname = 'myschema';
set search_path to '$user','public','myschema';
SELECT * FROM PG_TABLE_DEF where schemaname = 'myschema';

--Testing exlain
create table event (--from query given above)
load data into table (--as shown above using file 'allevents_pipe.txt'

select eventname,count(*) from event group by eventname;
explain select eventname,count(*) from event group by eventname;
enable explain from menu on top to look into graph.

--create venue table if not created already.
create table venue(
	venueid smallint not null distkey sortkey,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer);
note** usage of distkey and sortkey

--
create table venue1(
	venueid smallint not null ,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer)
    sortkey(venueid);

--
create table venue2(
	venueid smallint not null ,
	venuename varchar(100),
	venuecity varchar(30),
	venuestate char(2),
	venueseats integer)
 sortkey(venueid,venuecity);

select * from pg_table_def where tablename = 'venue2';
--look at columns distkey and sortkey(for value other than 0)

COPY dev.public.venue 
FROM 's3://buck1mar/venue_pipe.txt' 
IAM_ROLE 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502' 
FORMAT AS CSV DELIMITER '|'
REGION AS 'eu-central-1'

elapsed time: 1.6s

load same data into venue2 and check elapsed time (~25s)
--second run : 13s
select count(*) from venue;
select count(*) from venue2;


select * from venue where venueid = 7 and venuecity = 'Toronto';
xxxx ms
select * from venue2 where venueid = 7 and venuecity = 'Toronto';
yyyy ms (lesser than xxxx?)

--multiple runs reduces the time taken..

create table venue3(
	venueid smallint not null encode az64 ,
	venuename varchar(100) encode bytedict,
	venuecity varchar(30) encode bytedict,
	venuestate char(2) encode lzo,
	venueseats integer encode az64)
 sortkey(venueid,venuecity);

select * from pg_table_def where tablename = 'venue3';

#--create listing table
create table listing(
	listid integer not null distkey,
	sellerid integer not null,
	eventid integer not null,
	dateid smallint not null  sortkey,
	numtickets smallint not null,
	priceperticket decimal(8,2),
	totalprice decimal(8,2),
	listtime timestamp);

create table listing2(
	listid integer not null,
	sellerid integer not null,
	eventid integer not null,
	dateid smallint not null  sortkey,
	numtickets smallint not null,
	priceperticket decimal(8,2),
	totalprice decimal(8,2),
	listtime timestamp) diststyle key distkey (sellerid);

--Load data into both tables
COPY dev.public.listing2 FROM 's3://buck1mar/listings_pipe.txt' 
IAM_ROLE 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502' 
FORMAT AS CSV DELIMITER '|' QUOTE '"' REGION AS 'eu-central-1'

Select diststyle, skew_rows from svv_table_info ;
Select diststyle, skew_rows from svv_table_info where "table" = 'listing2';

--test querying on bth tables..
---------
--Unload data to S3
unload ('select * from listing') to 's3://buck1mar/listing_bckup'
iam_role 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502'
json

Note** creates multiple files here with prefix as listing_backup..
Amazon Redshift splits the results of a select statement across a set of files, one or more files per node slice, 
to simplify parallel reloading of the data. Alternatively, you can specify that UNLOAD should write the results serially to 
one or more files by adding the PARALLEL OFF option. You can limit the size of the files in Amazon S3 by specifying the MAXFILESIZE parameter. 
UNLOAD automatically encrypts data files using Amazon S3 server-side encryption (SSE-S3).

By default, UNLOAD writes data in parallel to multiple files, according to the number of slices in the cluster

unload ('select * from listing') to 's3://buck1mar/listing_bckup2'
iam_role 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502'
parallel off

other options:
--to divide data into equal size files
maxfilesize xx mb;

--to create a manifest file that lists the unload files
The manifest is a text file in JSON format that explicitly lists the URL of each file that was written to Amazon S3.
manifest

--UNLOAD command includes a quoted string in the select statement, so the quotation marks are escaped (=\'OH\' ').
unload ('select venuename, venuecity from venue where venuestate=\'OH\' ')
to 's3://buck1mar/listing_bckup'
iam_role 'arn:aws:iam::381492072383:role/service-role/AmazonRedshift-CommandsAccessRole-20240313T180502'

--to overwrite existing files
allowoverwrite

==========================
System catelog tables
PG_*

System monitoring tables  (but for provisioned only clusters)

#STL views are generated from logs that have been persisted to disk to provide a history of the system.
#STV tables are virtual system tables that contain snapshots of the current system data. 
 They are based on transient in-memory data and are not persisted to disk-based logs or regular tables.

--using STV tables
--cancelling a long running query
select pid,trim(user_name),starttime,substring(query,1,20) from stv_recents where status = 'Running';
cancel pid;
abort or rollback;

--check if following permissions are added (if needed)
AmazonRedshiftQueryEditorV2FullAccess

#SVCS views provide details about queries on both the main and concurrency scaling clusters.

#SVL views provide details about queries on main clusters.

#SYS - monitoring views
used to monitor query and workload usage for provisioned clusters and serverless workgroups.

#SVV - metadata views
Contain information about database objects with references to transient STV tables.
select * from svv_tables ;
select * from svv_tables where table_name like 'venue%';


























